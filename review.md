# Learning robust, real-time, reactive robotic grasping

## Abstract

**기존 로봇 grasp 기술(Open-loop grasping)**: 
1. image 상 grasp 후보를 선정해 일일이 sampling 및 후보에 대한 grasp 성공 가능성 점수를 통해 순위를 매기는 분류 방식. -> 계산 시간 오래걸림.(실시간에 부적합)
2. 처음에 한 번 grasp 계획을 세우고 나면 고정된 goal position으로만 이동하기 때문에 동적인 환경 및 정확하지 않은 센서, 제어 값에서 사용 부적합.

 **GG-CNN(Generative Grasping Convolutional Neural Network)**: 
 
 deep learning을 통해 input된 depth image의 각 pixel(카메라로부터 해당 위치의 물체까지의 거리)에 대해 grasp 품질(Quality)(성공 확률), grasp 각도(Angle), grasp 너비(Width)를 동시에 예측하고 
 object-independent grasp 수행(Cornell Grasping Dataset을 통해 label된 grasp rectangle을 pixel 단위의 map으로 변환하여 grasp 성공 확률이 높은 위치, 각도, 너비를 예측하도 학습, 각도는 주기성을 고려하여 sine 및 cosine으로 표)

 => 특징:
 - 가장 높은 품질의 pixel을 찾아 최적의 grasp 자세 결정. 낯선 물체 및 다양한 특성을 가진 물체, 동적인 환경(주변 환경이 실시간으로 변하거나 물체가 움직이는 환경), 센서 노이즈 및 제어 오차, 물체가 무질서하게 밀집되어 있거나 가려진 복잡한 환경(clutter)에서도 높은 그립 성공률(실시간)

실시간 반응성: 경량화된 네트워크로 빠른 추론이 가능하여 동적 환경에서도 효과적.

픽셀 단위 예측: 전체 이미지에 대해 그립 가능성을 예측하여 다양한 그립 위치 탐색 가능.

간단한 구조: 복잡한 후보 샘플링 없이도 높은 정확도의 그립 예측 가능.

일반화 능력: 미지의 객체나 다양한 환경에서도 안정적인 성능을 보임.
 
 => 장점:
 - grasp 대상 물체가 움직이거나 주변 환경이 예측 불가능하게 변화하는 동적 환경에서도 실시간 grasp 예측 가능.(model 경량화 및 속도 향상(초당 50Hz으로 depth image 받아 실시간으로 gripper 위치 update)
 - 정확도 요구 사항 완화(Camera 및 로봇 간의 정밀한 Calibration 이나 로봇 위치 제어의 정확도에 덜 의존적, 오차에도 작동)
 - grasp 동작 중에 센서로부터 실시간으로 주변 환경 정보를 받아들이고 이를 바탕으로 gripper의 움직임을 수정하여 goal grasping position 으로 전환(close-loop grasping)
 

 => 단점:
 - input 값이 depth image 이기 때문에 투명하거나 반사율이 높은 물체, 고도로 복잡한 표면에서는 성능 저하

 => 성능:
 - 정적 객체, 동적 객체, 혼잡 환경에서 모두 높은 grasp 성공률

## Contribution

1. 픽셀 단위의 실시간 그립 예측을 위한 GG-CNN 제안
기존의 그리핑 방법은 이미지에서 그립 후보를 샘플링하고 이를 평가하는 방식으로, 계산 비용이 높고 실시간 적용에 어려움이 있었습니다. 이에 반해, GG-CNN(Generative Grasping Convolutional Neural Network)은 입력된 깊이 이미지의 각 픽셀에 대해 그립 품질, 각도, 너비를 동시에 예측하여, 전체 이미지에 대한 그립 가능성을 실시간으로 평가합니다. 이러한 접근 방식은 복잡한 후보 샘플링 과정을 제거하고, 빠른 추론을 가능하게 합니다.

2. 경량화된 네트워크 구조로 실시간 폐루프 제어 구현
GG-CNN은 약 62,000개의 파라미터를 가진 경량화된 완전 합성곱 신경망(Fully Convolutional Network)으로, 약 19ms의 추론 시간으로 최대 50Hz의 실시간 폐루프 제어를 가능하게 합니다. 이러한 빠른 추론 속도는 동적 환경에서의 그립 수행에 필수적이며, 기존의 대형 네트워크 구조에 비해 하드웨어 요구사항을 크게 낮춥니다.

3. 동적 환경에서의 강인한 그립 성능 입증
실험을 통해 GG-CNN은 정적 객체, 동적 객체, 혼잡한 환경에서 모두 높은 그립 성공률을 보였습니다:

복잡한 형상의 미지 객체에 대해 84%의 성공률

가정용 물체에 대해 94%의 성공률

동적 환경에서 88%의 성공률

이러한 결과는 GG-CNN의 실시간 반응성과 일반화 능력을 입증합니다.

4. 다중 시점 기반의 그립 예측으로 혼잡 환경에서의 성능 향상
GG-CNN의 실시간 성능을 활용하여, 다양한 시점에서의 그립 예측을 통합하는 방법을 제안하였습니다. 이를 통해 가려진 영역이나 복잡한 배치의 물체에 대한 그립 성공률을 최대 10%까지 향상시킬 수 있었습니다.

5. GG-CNN2: 개선된 모델 아키텍처 및 학습 전략
기존 GG-CNN의 한계를 보완하기 위해 GG-CNN2를 제안하였습니다. 이 모델은 Jacquard 데이터셋을 활용하여 학습되었으며, 네트워크 구조와 학습 전략을 개선하여 더욱 향상된 성능을 보입니다.
ResearchGate

✅ 요약
이 논문은 경량화된 네트워크 구조를 통해 실시간 폐루프 제어를 가능하게 하고, 픽셀 단위의 그립 예측을 통해 동적 환경에서도 강인한 성능을 보이는 GG-CNN을 제안하였습니다. 또한, 다중 시점 기반의 그립 예측과 개선된 모델 아키텍처(GG-CNN2)를 통해 복잡한 환경에서도 높은 그립 성공률을 달성하였습니다.

## Method

최적 파지 자세- 로봇 그리퍼가 물체를 잡기 위해 도달해야 하는 3차원 공간에서의 위치(x,y,z), z축을 중심으로 한 회전 각도, 그리퍼가 벌려야할 너비

1. input & output
input: 300×300 pixel의 normalization된 depth image.

output: 각 pixel에 대해 다음을 예측하는 세 가지 맵:

Grasp Quality Map (Q): grasp 성공 확률(품질, quality) (0~1 사이의 값).

Grasp Angle Map (Φ): grasp angle (−π/2 ~ π/2 범위).

Grasp Width Map (W): grasp width (픽셀 단위).
ResearchGate

2. network architecture

구조: Fully Convolutional Network (FCN) 형태로, 약 62,000개의 파라미터를 가짐.

속도: 전체 파이프라인(전처리 포함) 추론 시간은 약 19ms로, 최대 50Hz의 실시간 제어 가능.

3. 학습 data 및 전처리

dataset: Cornell Grasping Dataset을 사용하여 학습.

전처리:

각 그립 사각형을 중심으로 하는 마스크를 생성하여 해당 영역의 Q, Φ, W 값을 설정.

각도 Φ는 주기성을 고려하여 sin(2Φ)와 cos(2Φ)로 변환하여 학습.

데이터 증강을 통해 다양한 시야와 깊이 변화에 대응.

4. 그립 예측 및 실행
그립 선택: Q 맵에서 가장 높은 값을 갖는 픽셀을 선택하여 해당 위치의 Φ와 W 값을 사용.

좌표 변환: 선택된 픽셀 좌표를 카메라와 로봇 간의 변환 행렬을 통해 월드 좌표로 변환.

실행 방식:

Open-loop: 단일 프레임에서 예측된 그립을 실행.

Closed-loop: 실시간으로 깊이 이미지를 받아 지속적으로 그립 위치를 업데이트하며 제어.

5. 성능 및 평가
정적 객체: 가정용 물체에 대해 92%의 성공률.

동적 객체: 움직이는 물체에 대해 88%의 성공률.

혼잡 환경: 복잡한 환경에서 87%의 성공률을 기록.


## Conclusion

1. 성과:

- 경량화로 인한 속도 향상: 다른 grasp network에 비해 크기가 작아 매우 빠른 계산 시간(최대 50Hz) 확보. 이로 인해 closed-loop control 가능

- 실시간성 확보:
  1. depth image로부터 pixel 단위로 grasp pose 정보 생성하는 방식을 통해 기존의 grasp 후보들을 일일이 Sampling 및 Classification 방식의 한계 극복.
  2. closed-loop grasping을 통해 동적 환경, 복잡한 환경에서 robust. 각각 88%, 87%의 높은 grasp 성공률
 


다중 시점 파지의 중요성: GG-CNN의 픽셀 단위 파지 예측 기능 덕분에 여러 시점에서 얻은 파지 정보들을 쉽게 결합할 수 있음을 보여줍니다. 이는 물체들이 복잡하게 쌓여있는 클러터(clutter) 환경에서 물체 가려짐(occlusion) 문제를 극복하고 파지 성공률을 최대 10%까지 향상시키는 데 효과적입니다.
네트워크 설계 및 훈련 분석: 실시간 실행을 위한 경량 네트워크 설계에는 성능과 계산 시간 사이의 균형이 필요함을 언급하며, 네트워크 설계 요소(예: 팽창된 합성곱)와 데이터 증강(data augmentation)의 중요성을 논합니다. 데이터 증강은 학습된 파지 알고리즘을 실제 로봇에 적용하는 데 필수적입니다.
향후 연구 방향: 현재 시스템은 특정 물체를 인지하지는 않지만, GG-CNN이 생성하는 파지 맵에 물체 또는 어포던스(affordances) 마스크를 적용하여 기능을 확장할 수 있음을 시사합니다. 또한, 능동적인 시점 선택, 촉각 센서 통합, 또는 미는(pushing) 동작과 같은 다른 조작 행동과의 결합을 통해 시스템의 강건성을 더욱 높일 수 있다고 제안합니다.

2. 한계 및 해결 방안:

- 특정 재질(검정색, 반사되는 물체, 투명한 물체)에 대한 정확한 depth information 추출 어려움.
  => multi-view 융합: 로봇이 여러 다른 위치나 경로를 따라 이동하면서 여러 장의 depth image 촬영 -> 각 image에서 GG-CNN으로 생성된 grip map 정보들을 수집-> 수집된 정보에서 각 격자 셀에 대해 여러 시점에서 관측된 파지 품질, 각도, 너비 정보들의 평균을 계산하여 최종 파지 후보를 결정

여러 시점에서 얻은 정보를 조합함으로써, 한 시점에서는 가려져 보이지 않았던 좋은 파지 지점 발견 가능.
다양한 각도에서 물체를 관찰하며 깊이 카메라의 측정 오류나 노이즈로 인한 부정확한 예측을 보완하고, 더 정확하고 신뢰할 수 있는 파지 정보를 얻을 수 있음.
  
- gripper의 구조로 물리적인 한계 발생: 얇은 물체, 미끄러지는 물체, 깨지기 쉬운 물체 등 잡기 어려움.
  => 촉각 센서와 같은 다중 센서 융합.
  
- 물체들이 밀집되어 있는 clutter 환경에서 gripper가 주변 물체와 충돌 및 grasp 실패 현상 발생.
  => grasp 뿐만 아니라 pushing와 같은 다른 조작 action 학습.
