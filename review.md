# Learning robust, real-time, reactive robotic grasping

## Abstract

**기존 로봇 grasp 기술(Open-loop grasping)**: 
1. image 상 grasp 후보를 선정해 일일이 sampling 및 후보에 대한 grasp 성공 가능성 점수를 통해 순위를 매기는 분류 방식. -> 계산 시간 오래걸림.(실시간에 부적합)
2. 처음에 한 번 grasp 계획을 세우고 나면 고정된 goal position으로만 이동하기 때문에 동적인 환경 및 정확하지 않은 센서, 제어 값에서 사용 부적합.

 **GG-CNN(Generative Grasping Convolutional Neural Network)**: 
 
 deep learning을 통해 input된 depth image의 각 pixel(카메라로부터 해당 위치의 물체까지의 거리)에 대해 grasp 품질(Quality)(성공 확률), grasp 각도(Angle), grasp 너비(Width)를 동시에 예측하고 
 object-independent grasp 수행(Cornell Grasping Dataset을 통해 label된 grasp rectangle을 pixel 단위의 map으로 변환하여 grasp 성공 확률이 높은 위치, 각도, 너비를 예측하도 학습, 각도는 주기성을 고려하여 sine 및 cosine으로 표)

 => 특징:
 - 가장 높은 품질의 pixel을 찾아 최적의 grasp 자세 결정. 낯선 물체 및 다양한 특성을 가진 물체, 동적인 환경(주변 환경이 실시간으로 변하거나 물체가 움직이는 환경), 센서 노이즈 및 제어 오차, 물체가 무질서하게 밀집되어 있거나 가려진 복잡한 환경(clutter)에서도 높은 그립 성공률(실시간)

실시간 반응성: 경량화된 네트워크로 빠른 추론이 가능하여 동적 환경에서도 효과적.

픽셀 단위 예측: 전체 이미지에 대해 그립 가능성을 예측하여 다양한 그립 위치 탐색 가능.

간단한 구조: 복잡한 후보 샘플링 없이도 높은 정확도의 그립 예측 가능.

일반화 능력: 미지의 객체나 다양한 환경에서도 안정적인 성능을 보임.
 
 => 장점:
 - grasp 대상 물체가 움직이거나 주변 환경이 예측 불가능하게 변화하는 동적 환경에서도 실시간 grasp 예측 가능.(model 경량화 및 속도 향상(초당 50Hz으로 depth image 받아 실시간으로 gripper 위치 update)
 - 정확도 요구 사항 완화(Camera 및 로봇 간의 정밀한 Calibration 이나 로봇 위치 제어의 정확도에 덜 의존적, 오차에도 작동)
 - grasp 동작 중에 센서로부터 실시간으로 주변 환경 정보를 받아들이고 이를 바탕으로 gripper의 움직임을 수정하여 goal grasping position 으로 전환(close-loop grasping)
 

 => 단점:
 - input 값이 depth image 이기 때문에 투명하거나 반사율이 높은 물체, 고도로 복잡한 표면에서는 성능 저하

 => 성능:
 - 정적 객체, 동적 객체, 혼잡 환경에서 모두 높은 grasp 성공률

## Contribution

1. 픽셀 단위의 실시간 그립 예측을 위한 GG-CNN 제안
기존의 그리핑 방법은 이미지에서 그립 후보를 샘플링하고 이를 평가하는 방식으로, 계산 비용이 높고 실시간 적용에 어려움이 있었습니다. 이에 반해, GG-CNN(Generative Grasping Convolutional Neural Network)은 입력된 깊이 이미지의 각 픽셀에 대해 그립 품질, 각도, 너비를 동시에 예측하여, 전체 이미지에 대한 그립 가능성을 실시간으로 평가합니다. 이러한 접근 방식은 복잡한 후보 샘플링 과정을 제거하고, 빠른 추론을 가능하게 합니다.

2. 경량화된 네트워크 구조로 실시간 폐루프 제어 구현
GG-CNN은 약 62,000개의 파라미터를 가진 경량화된 완전 합성곱 신경망(Fully Convolutional Network)으로, 약 19ms의 추론 시간으로 최대 50Hz의 실시간 폐루프 제어를 가능하게 합니다. 이러한 빠른 추론 속도는 동적 환경에서의 그립 수행에 필수적이며, 기존의 대형 네트워크 구조에 비해 하드웨어 요구사항을 크게 낮춥니다.

3. 동적 환경에서의 강인한 그립 성능 입증
실험을 통해 GG-CNN은 정적 객체, 동적 객체, 혼잡한 환경에서 모두 높은 그립 성공률을 보였습니다:

복잡한 형상의 미지 객체에 대해 84%의 성공률

가정용 물체에 대해 94%의 성공률

동적 환경에서 88%의 성공률
Monash University
arXiv

이러한 결과는 GG-CNN의 실시간 반응성과 일반화 능력을 입증합니다.

4. 다중 시점 기반의 그립 예측으로 혼잡 환경에서의 성능 향상
GG-CNN의 실시간 성능을 활용하여, 다양한 시점에서의 그립 예측을 통합하는 방법을 제안하였습니다. 이를 통해 가려진 영역이나 복잡한 배치의 물체에 대한 그립 성공률을 최대 10%까지 향상시킬 수 있었습니다.

5. GG-CNN2: 개선된 모델 아키텍처 및 학습 전략
기존 GG-CNN의 한계를 보완하기 위해 GG-CNN2를 제안하였습니다. 이 모델은 Jacquard 데이터셋을 활용하여 학습되었으며, 네트워크 구조와 학습 전략을 개선하여 더욱 향상된 성능을 보입니다.
ResearchGate

✅ 요약
이 논문은 경량화된 네트워크 구조를 통해 실시간 폐루프 제어를 가능하게 하고, 픽셀 단위의 그립 예측을 통해 동적 환경에서도 강인한 성능을 보이는 GG-CNN을 제안하였습니다. 또한, 다중 시점 기반의 그립 예측과 개선된 모델 아키텍처(GG-CNN2)를 통해 복잡한 환경에서도 높은 그립 성공률을 달성하였습니다.

## Method

1. input & output
input: 300×300 pixel의 normalization된 depth image.

output: 각 pixel에 대해 다음을 예측하는 세 가지 맵:

Grasp Quality Map (Q): grasp 성공 확률(품질, quality) (0~1 사이의 값).

Grasp Angle Map (Φ): grasp angle (−π/2 ~ π/2 범위).

Grasp Width Map (W): grasp width (픽셀 단위).
ResearchGate

2. network architecture

구조: Fully Convolutional Network (FCN) 형태로, 약 62,000개의 파라미터를 가짐.

속도: 전체 파이프라인(전처리 포함) 추론 시간은 약 19ms로, 최대 50Hz의 실시간 제어 가능.

3. 학습 data 및 전처리

dataset: Cornell Grasping Dataset을 사용하여 학습.

전처리:

각 그립 사각형을 중심으로 하는 마스크를 생성하여 해당 영역의 Q, Φ, W 값을 설정.

각도 Φ는 주기성을 고려하여 sin(2Φ)와 cos(2Φ)로 변환하여 학습.

데이터 증강을 통해 다양한 시야와 깊이 변화에 대응.

4. 그립 예측 및 실행
그립 선택: Q 맵에서 가장 높은 값을 갖는 픽셀을 선택하여 해당 위치의 Φ와 W 값을 사용.

좌표 변환: 선택된 픽셀 좌표를 카메라와 로봇 간의 변환 행렬을 통해 월드 좌표로 변환.

실행 방식:

Open-loop: 단일 프레임에서 예측된 그립을 실행.

Closed-loop: 실시간으로 깊이 이미지를 받아 지속적으로 그립 위치를 업데이트하며 제어.

5. 성능 및 평가
정적 객체: 가정용 물체에 대해 92%의 성공률.

동적 객체: 움직이는 물체에 대해 88%의 성공률.

혼잡 환경: 복잡한 환경에서 87%의 성공률을 기록.


## Conclusion

1. 성과:

- 경량화로 인한 속도 향상: 다른 grasp network에 비해 크기가 작아 매우 빠른 계산 시간(최대 50Hz) 확보. 이로 인해 closed-loop control 가능

- 실시간성 확보:
  1. depth image로부터 pixel 단위로 grasp pose 정보 생성하는 방식을 통해 기존의 grasp 후보들을 일일이 Sampling 및 Classification 방식의 한계 극복.
  2. closed-loop grasping을 통해 동적 환경, 복잡한 환경에서 robust. 각각 88%, 87%의 높은 grasp 성공률
 


다중 시점 파지의 중요성: GG-CNN의 픽셀 단위 파지 예측 기능 덕분에 여러 시점에서 얻은 파지 정보들을 쉽게 결합할 수 있음을 보여줍니다. 이는 물체들이 복잡하게 쌓여있는 클러터(clutter) 환경에서 물체 가려짐(occlusion) 문제를 극복하고 파지 성공률을 최대 10%까지 향상시키는 데 효과적입니다.
네트워크 설계 및 훈련 분석: 실시간 실행을 위한 경량 네트워크 설계에는 성능과 계산 시간 사이의 균형이 필요함을 언급하며, 네트워크 설계 요소(예: 팽창된 합성곱)와 데이터 증강(data augmentation)의 중요성을 논합니다. 데이터 증강은 학습된 파지 알고리즘을 실제 로봇에 적용하는 데 필수적입니다.
향후 연구 방향: 현재 시스템은 특정 물체를 인지하지는 않지만, GG-CNN이 생성하는 파지 맵에 물체 또는 어포던스(affordances) 마스크를 적용하여 기능을 확장할 수 있음을 시사합니다. 또한, 능동적인 시점 선택, 촉각 센서 통합, 또는 미는(pushing) 동작과 같은 다른 조작 행동과의 결합을 통해 시스템의 강건성을 더욱 높일 수 있다고 제안합니다.

2. 한계 및 해결 방안:

- 특정 재질(검정색, 반사되는 물체, 투명한 물체)에 대한 정확한 depth information 추출 어려움.
  => 센서 융합.(ex:RGB+Depth)
- gripper의 구조로 물리적인 한계 발생: 얇은 물체, 미끄러지는 물체, 깨지기 쉬운 물체 등 잡기 어려움.
  => 촉각 센서와 같은 다중 센서 융합.
  
- 물체들이 밀집되어 있는 clutter 환경에서 gripper가 주변 물체와 충돌 및 grasp 실패 현상 발생.
  => grasp 뿐만 아니라 pushing와 같은 다른 조작 action 학습.

  















   다중 시점(multi-view) 융합 방법
요약하자면, 이 논문은 딥러닝을 통해 깊이 이미지만으로 물체의 **픽셀 단위 파지 맵(pixel-wise grasp map)**을 실시간으로 생성하는 새로운 신경망 모델(GG-CNN)을 제안하고, 이를 폐쇄 루프 방식으로 로봇 제어에 활용하여 처음 보는 물체, 움직이는 물체, 복잡한 환경 등 다양한 도전적인 상황에서도 강건하게 파지를 수행하는 방법을 제시합니다.

최적 파지 자세- 로봇 그리퍼가 물체를 잡기 위해 도달해야 하는 3차원 공간에서의 위치(x,y,z), z축을 중심으로 한 회전 각도, 그리퍼가 벌려야할 너비


로봇이 물체가 복잡하게 쌓여 있는 '클러터(clutter)' 환경에서 파지할 때, 한 시점에서는 다른 물체에 가려져(occlusion) 파지에 적합한 물체의 특정 부분이 보이지 않을 수 있습니다.
또한, 깊이 카메라 자체의 한계로 인해 특정 재질(예: 반사되거나 투명하거나 검은색 물체)에 대한 깊이 정보가 부정확하거나 누락될 수 있습니다.


다중 시점 방식은 어떻게 작동하나요?
이 논문에서 제안하는 GG-CNN 모델은 깊이 이미지를 입력받아 해당 시점에서 볼 수 있는 모든 픽셀에 대해 파지 품질, 각도, 너비 등의 정보를 예측하는 '픽셀 단위 파지 맵'을 생성합니다.
다중 시점 방식에서는 로봇이 여러 다른 위치나 경로를 따라 이동하면서 여러 장의 깊이 이미지를 촬영하고, 각 이미지에서 GG-CNN으로 생성된 파지 맵 정보들을 수집합니다.
수집된 여러 시점의 파지 정보들은 작업 공간을 격자 형태로 나눈 '그립 맵(grip map)'에 누적되어 조합됩니다. 각 격자 셀에 대해 여러 시점에서 관측된 파지 품질, 각도, 너비 정보들의 평균을 계산하여 최종 파지 후보를 결정합니다.


다중 시점 방식이 파지 성공률을 어떻게 높이나요?
여러 시점에서 얻은 정보를 조합함으로써, 한 시점에서는 가려져 보이지 않았던 좋은 파지 지점을 발견할 수 있습니다.
다양한 각도에서 물체를 관찰하며 깊이 카메라의 측정 오류나 노이즈로 인한 부정확한 예측을 보완하고, 더 정확하고 신뢰할 수 있는 파지 정보를 얻을 수 있습니다.
특히 클러터 환경에서 물체들 사이에 숨겨진 최적의 파지 지점을 찾아내거나, 잘못된 파지 예측(false positive) 가능성이 있는 영역의 신뢰도를 낮추는 효과를 얻을 수 있습니다.


모델의 효율성 및 성능: 이 네트워크는 다른 최신 기술에 비해 모델 크기가 훨씬 작으면서도(orders of magnitude smaller), 특히 **클러터(clutter, 물체가 빽빽하게 쌓인 환경)**에서 더 나은 성능을 보여줍니다.


  한 번의 네트워크 추론으로 영상 전체에서 가능한 모든 그리핑을 평가할 수 있다. 이 방식은 기존의 후보 샘플링-평가 방식보다 훨씬 빠르고, 실시간 제어에 적합하다. 네트워크 구조는 매우 경량화되어(약 62,000개 파라미터) GPU가 탑재된 임베디드 시스템에서도 동작






## 방법론
현실 공간에서의 파지 정의 (\(g\)):

논문은 로봇이 물체를 잡는 '파지(grasp)'를 \(g = (q, p, \phi, w)\) 라는 네 가지 정보의 조합으로 정의합니다. 여기서 로봇의 그리퍼는 x-y 평면에 수직으로 내려가서 물체를 잡는다고 가정합니다.
\(q\) (품질, quality): 파지 성공 확률을 나타내는 0과 1 사이의 스칼라 값입니다. 1에 가까울수록 성공 가능성이 높습니다.
\(p = (x, y, z)\) (위치, position): 월드 좌표계(로봇 기준)에서의 그리퍼 중심 위치입니다.
\(\phi\) (각도, rotation): z축을 중심으로 한 그리퍼의 회전 각도입니다.
\(w\) (너비, width): 파지를 위해 필요한 그리퍼의 벌어진 너비입니다. 기존 연구들 중에는 그리퍼 너비를 고정하거나 고려하지 않는 경우가 많았지만, 이 논문에서는 너비 예측을 통해 클러터 환경 등에서 성능 향상을 도모합니다.


이미지 공간에서의 파지 정의 (\(\tilde{g}\)):

입력으로 사용되는 깊이 이미지 \(I\) (높이 H, 너비 W) 상에서 파지는 \(\tilde{g} = (q, \tilde{s}, \tilde{\phi}, \tilde{w})\) 로 정의됩니다.
\(q\) (품질, quality): 현실 공간과 동일하게 파지 성공 확률을 나타냅니다.
\(\tilde{s} = (u, v)\) (중심점, center point): 이미지 좌표계(픽셀)에서의 파지 중심점 위치입니다.
\(\tilde{\phi}\) (각도, rotation): 카메라 기준 좌표계에서의 회전 각도입니다. 반대칭 파지(antipodal grasp)의 특성 상, 각도는 \(\left[ -\frac{\pi}{2}, \frac{\pi}{2} \right]\) 범위 내에 있습니다. (논문에서는 이 각도를 \(\sin(2\tilde{\phi})\)와 \(\cos(2\tilde{\phi})\) 두 개의 요소로 인코딩하여 신경망 학습을 용이하게 합니다.)
\(\tilde{w}\) (너비, width): 이미지 좌표계에서의 파지 너비입니다 (픽셀 단위, 최대 150 픽셀). 이는 카메라 파라미터와 해당 픽셀의 깊이를 이용해 현실 공간의 물리적 너비로 변환할 수 있습니다.


이미지 공간 파지를 현실 공간 파지로 변환하는 과정:

깊이 이미지 상에서 정의된 파지 \(\tilde{g}\)는 알려진 변환 과정을 거쳐 현실 공간의 파지 \(g\)로 변환됩니다. 이 과정은 아래의 수식 (1)로 표현됩니다.
수식 (1):
\[ g = t_{RC} (t_{CI} (\tilde{g})) \quad (1) \]
\(g\): 현실 공간(월드/로봇 프레임)에서의 파지 자세입니다.
\(t_{RC}\): 카메라 프레임에서 월드/로봇 프레임으로 변환하는 과정입니다.
\(t_{CI}\): 2차원 이미지 좌표 및 깊이 정보(\(\tilde{g}\)에 포함)를 3차원 카메라 프레임 좌표로 변환하는 과정입니다. 카메라 고유 파라미터와 로봇-카메라 간의 보정 정보를 이용합니다.
\(\tilde{g}\): 2차원 깊이 이미지 상에서 정의된 파지 정보입니다.
파지의 깊이(\(z\) 값)는 그리퍼 손가락 끝이 측정된 깊이와 일치하도록 일정량의 오프셋이 적용됩니다.


파지 맵 정의 (\(\tilde{G}\)):

논문에서는 GG-CNN이 예측하는 최종 결과물을 '파지 맵(grasp map)' \(\tilde{G}\)로 정의합니다. 이는 이미지 공간에서의 파지 정보들을 모아놓은 것으로, \(\tilde{G} = (\tilde{Q}, \tilde{F}, \tilde{W}) \in \mathbb{R}^{3 \times H \times W}\) 형태를 가집니다.
\(\tilde{Q}\): 각 픽셀 \((u, v)\)에서의 파지 품질 \(q\)를 나타내는 HxW 크기의 이미지입니다.
\(\tilde{F}\): 각 픽셀 \((u, v)\)에서의 파지 각도 \(\tilde{\phi}\)를 나타내는 HxW 크기의 이미지입니다. (실제로는 \(\sin(2\tilde{\phi})\)와 \(\cos(2\tilde{\phi})\) 채널로 구성됩니다.)
\(\tilde{W}\): 각 픽셀 \((u, v)\)에서의 필요한 그리퍼 너비 \(\tilde{w}\)를 나타내는 HxW 크기의 이미지입니다.
즉, 파지 맵은 깊이 이미지의 각 픽셀에 대해 해당 위치에서 파지를 시도할 때의 품질, 각도, 너비를 조밀하게 담고 있습니다.


최적 파지 도출:

신경망은 깊이 이미지 \(I\)를 입력받아 함수 \(M\)을 통해 파지 맵 \(\tilde{G}\)를 예측합니다. \(M(I) = \tilde{G}\).
이미지 공간에서 가장 품질이 높은 파지 \(\tilde{g}^*\)는 파지 맵 \(\tilde{G}\) 중 \(\tilde{Q}\) 값이 최대인 픽셀 위치에서 얻어지며, 해당 위치의 \(\tilde{F}\)와 \(\tilde{W}\) 값을 사용합니다.
이후 수식 (1)을 적용하여 현실 공간에











Contribution (기여)
기술적 혁신

픽셀 단위 예측 (Pixel-wise Grasp Prediction):

기존의 Sample & Rank 방식(후보 그리핑 포즈 샘플링 및 순위 매김)을 배제하고, 깊이 영상의 모든 픽셀에서 그리핑 품질(Q), 각도(Φ), 너비(W)를 직접 예측.

19ms 내 추론 가능 (NVIDIA GTX 1070 GPU 기준), 기존 대비 50Hz 폐루프 제어 구현.

각도 대칭성 해결:

각도 예측 시 **sin(2Φ)**와 **cos(2Φ)**로 분해하여 ±π/2 모호성 제거.

최종 각도 계산:
Φ
=
1
2
arctan
⁡
(
sin
⁡
(
2
Φ
)
cos
⁡
(
2
Φ
)
)
Φ= 
2
1
 arctan( 
cos(2Φ)
sin(2Φ)
 )

깊이 불변성 (Depth Invariance):

깊이 영상 정규화로 카메라 거리 변화에 강건한 예측 가능.

실용적 기여

경량 네트워크: 62,420개 파라미터 (기존 대비 1/1000 수준), <1MB 모델 크기로 임베디드 시스템(NVIDIA TX2) 배포 가능.

다중 뷰 융합 (Multi-view Fusion):

복잡한 환경에서 그리핑 성공률 10% 향상 (단일 뷰 대비).

실시간 동적 보정:

이동 중인 물체(최대 20cm/s 속도)에 대해 88% 성공률 달성.

오픈소스 공개

GitHub 리포지토리(링크) 공개로 후속 연구 활성화.

Method (방법론)
네트워크 구조

입력: 320×320 깊이 영상 (정규화 후).

출력: 3채널 Grasp Map (Quality, Angle, Width).

계층:

합성곱 계층 (3층): 3×3 커널로 특징 추출.

전치 합성곱 계층 (3층): 특징 맵 업샘플링.

각도 예측: sin(2Φ)와 cos(2Φ) 분해로 대칭성 문제 해결.

학습 전략

데이터셋: Cornell Grasping Dataset (885개 이미지, 5,110개 양성 레이블).

증강 기법:

무작위 회전(±30°), 크롭, 노이즈 추가.

손실 함수:

L2 Loss로 Q, sin(2Φ), cos(2Φ), W 예측 최적화.

실시간 폐루프 제어

50Hz 추론 속도: Intel RealSense 카메라로 실시간 깊이 영상 획득.

동적 보정 메커니즘:

그리퍼 접근 중 객체 이동 시 실시간 예측 업데이트.

후처리:

가우시안 필터로 Q 맵 노이즈 제거 → 최대 Q 값 위치 선택.

실험 환경

로봇 플랫폼: Kinova Mico 6-DoF 팔 + 평행 그리퍼.

객체 세트:

Household: 일상용품 (머그컵, 공구 등).

Adversarial: 투명/변형 가능/반사체.

Viereck: 표준 벤치마크 객체.

Conclusion (결론)
주요 성과

정적 환경:

Household 객체 94%, Adversarial 객체 84%, Viereck 벤치마크 100% 성공률.

동적 환경:

움직이는 Household 객체 88%, Adversarial 객체 83% 성공률.

혼잡 환경:

다중 뷰 융합으로 10% 향상된 87% 성공률.

의의

실시간 폐루프 제어를 통한 동적 환경 대응력 입증.

경량 네트워크로 엣지 디바이스 배포 가능성 확인 (산업용 로봇 적용 용이).

한계 및 향후 과제

한계:

투명체, 구형 객체 등 비평면 표면에서 성능 저하.

깊이 센서 최소 측정 거리(150mm) 미만에서 정확도 감소.

향후 방향:

RGB-D 센서 융합 → 텍스처 없는 객체 대응력 향상.

촉각 피드백 통합 → 그리핑 후 안정성 평가.

다지 그리퍼 확장 연구.
