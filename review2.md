KGNv2: Separating Scale and Pose Prediction for Keypoint-based 6-DoF Grasp Synthesis on RGB-D input

본 논문은 RGB-D 이미지에서 로봇 그리퍼의 6-DoF(6 자유도) 그래스프 포즈를 추정하는 개선된 키포인트 기반 접근 방식인 KGNv2를 제안하고 있습니다. 새로운 방법론적 접근을 통해 정확성과 일반화 능력을 향상시켰으며, 다양한 객체 형태와 환경에서 강건한 성능을 보여줍니다.

Abstract
KGNv2- input: RGB-D(2D RGB image + 3D Depth image), output: keypoint 검출을 통한 6-DoF grasp pose & gripper open width prediction
연구자들은 2D/2.5D 입력에서 키포인트를 기반으로 한 새로운 6-DoF 그래스프 포즈 합성 접근 방식을 제안했습니다. 이미지 입력을 사용한 키포인트 기반 그래스프 감지 방법은 이전 연구에서 유망한 결과를 보여주었지만, 이미지 공간에서 키포인트의 정확한 위치 예측에 크게 의존합니다. 이러한 문제를 해결하기 위해 정확한 키포인트 예측에 대한 의존성을 줄이는 새로운 그래스프 생성 네트워크를 개발했습니다.

KGNv2는 RGB-D 입력이 주어지면 키포인트 감지를 통한 그래스프 포즈와 카메라 방향의 스케일을 모두 예측합니다. 또한 키포인트 출력 공간을 재설계하여 Perspective-n-Point(PnP) 알고리즘에 대한 키포인트 예측 노이즈의 부정적 영향을 완화했습니다. 이러한 간단한 수정은 실험에서 기준 모델인 KGN보다 크게 향상된 성능을 보여주었으며, 단순한 합성 객체로만 훈련했음에도 실제 물체와 환경에서 잘 일반화됨을 입증했습니다.

Contributions

1. 네트워크 분리를 통한 Scale & grasp pose 별도 예측
원래의 KGN 모델은 키포인트 근접성 예측에 크게 의존했으며, 이는 특히 합성 데이터에서 학습하고 실제 환경에서 테스트할 때 불안정한 결과를 초래했습니다. KGNv2는 포즈와 스케일을 별도로 예측함으로써 정확한 키포인트 근접성 추정의 필요성을 제거했습니다.

2. 스케일 정규화 키포인트 설계
키포인트 출력 공간을 추정된 스케일로 정규화하도록 재설계하여 키포인트 오류에 대한 민감성을 줄이고 추정된 포즈의 정밀도를 향상시켰습니다. PnP 알고리즘에 대한 수치 분석을 통해 스케일이 큰 그래스프 포즈(카메라에서 더 멀리 있는 포즈)가 노이즈에 더 민감하다는 것을 발견하고, 이를 해결하기 위한 설계를 제안했습니다.

3. 성능 향상 및 일반화
제안된 수정 사항들은 간단하지만 효과적이어서, Primitive Shape 데이터셋의 모든 테스트에서 그래스프 예측 성능을 크게 향상시켰습니다. 특히 복잡한 다중 객체 환경에서 GSR, GCR, OSR 메트릭에서 각각 27.8%, 17.8%, 6.6%의 성능 향상을 달성했습니다.

방법론(Method)
문제 정의
단안 RGB-D 입력이 주어졌을 때, 입력을 3D 표현(예: 포인트 클라우드)으로 변환하지 않고 6-DoF 그래스프 포즈 g ∈ SE(3)와 관련 그리퍼 개방 폭 w를 합성하는 것이 목표입니다.

주요 컴포넌트
1. 스케일 정규화 키포인트를 통한 포즈 추정
KGNv2는 키포인트 기반 전략을 채택하여 카메라 3D-2D 투영 원리를 활용해 스케일까지의 그래스프 포즈를 추정합니다. 네트워크는 다음을 예측합니다:

각 방향 간격에 대한 그래스프 중심 집합 {c^m_i}

중심-키포인트 오프셋 벡터 맵 O

이 설계는 여러 그래스프를 동시에 감지할 수 있게 하고, 회전 대칭 객체에 대한 다양한 후보군 생성에 유용합니다.

스케일 정규화 키포인트 설계는 다음과 같습니다:

text
O_c = Õ_c / S_c
여기서 Õ_c는 실제 오프셋 벡터이고, S_c는 예측된 스케일입니다.

2. 스케일 예측
KGNv2는 각 픽셀과 방향에 대해 직접 스케일 맵 S ∈ R^(H'×W'×M)을 예측합니다. 예측된 스케일을 사용하면 PnP 알고리즘에서 얻은 변환 크기를 쉽게 개선할 수 있습니다:

text
ĝ = {R̂, T̂} = {R̃, (S_c,m / ||T̃||) · T̃}
여기서 R̃과 T̃는 PnP 알고리즘에서 얻은 회전과 변환입니다.

3. 네트워크 아키텍처 및 학습
비전 인코더로 DLA-34 사용 (4채널 RGB-D 입력용으로 수정)

각 작업 헤드에 얕은 2층 컨볼루션 네트워크 사용

최종 손실 함수:

text
L = γY·LY + γO·LO + γW·LW + γS·LS
(γY = 1, γO = 1, γW = 10, γS = 10)

실험 결과 및 결론
합성 데이터셋 실험
Primitive Shape 데이터셋에서 KGNv2는 모든 설정에서 기준 KGN을 크게 능가했습니다:

다중 객체 데이터로 학습하고 테스트했을 때, 가장 엄격한 임계값 조건에서 GSR, GCR, OSR에서 각각 27.8%, 17.8%, 6.6%의 성능 향상

단일 객체 장면에서 학습하고 더 복잡한 다중 객체 시나리오에서 테스트했을 때, 10.6%, 9.8%, 14.2%의 성능 향상을 기록

절제 연구를 통해 두 가지 주요 수정 사항(스케일 분기와 스케일 정규화 키포인트)이 모두 성능 향상에 기여한다는 것을 입증했습니다.

실제 로봇 실험
실제 환경에서의 실험을 통해 합성-현실 격차에 대한 일반화 능력을 검증했습니다:

단일 객체 그래스핑: 92.5 ± 6.7% 성공률 (KGN의 87.5 ± 9.6%보다 향상)

다중 객체 그래스핑: 80 ± 10.1% 성공률, 96 ± 7.4% 정리율

conclusion

KGNv2: 
- 요약: rgb-d image를 입력으로 받아 6-dof 파지 자세 추정
- 방법론: image에서 keypoint 탐지 및 PnP 알고리즘을 사용하여 파지 자세 추정(Scale & gripper open width는 분리된 네트워크로 별도 regress하여 예측)
- 개선점 및 효과 : PnP 알고리즘에 대한 분석을 바탕으로 Scale-normalized keypoint 디자인 도입 => 파지 자세 추정 정확도 높임.

- 검증: 합성 데이터셋을 사용한 실험에서 kgnv2가 기존 kgn 방식보다 레이블로부터 파지 분포 더 잘 학습함을 확인,
sim-to real 검증, 높은 grasp 성공률

-한계: 실제 실험에서 단일 객체 파지 시 불안정한 자세 예측이나 네트워크의 부적절한 외삽으로 인한 가려진 영역 파지 시도 등 실패 사례가 발생했습니다. 
또한, 학습에 사용된 단순한 합성 객체들의 균일한 색상 특성이 실제 세계의 다양하고 복잡한 시각적 외형을 
인식하는 모델의 능력에 한계를 줄 수 있다고 언급합니다.

-향후 연구 방향: 향후 연구로는 생성 모델(diffusion model, TEXTure 등)을 활용하여 
학습 데이터셋에 실제와 유사한 다양한 질감을 추가하는 방안을 모색 가능

본 논문에서 제안한 KGNv2는 키포인트와 PnP 알고리즘을 활용하면서도 스케일 예측을 분리하고 
키포인트 표현 방식을 개선하여 6자유도 파지 자세 추정의 정확도를 높였습니다. 
특히, 합성 데이터만으로 학습하여도 실제 환경에서 경쟁력 있는 성능을 보인다는 점에서 로봇 조작 분야에 기여하는 바가 큽니다. 
다만, 복잡한 객체 외형이나 가려진 영역에 대한 강건성 등 여전히 개선의 여지가 남아 있으며, 이는 후속 연구에서 탐구될 수 있습니다.
